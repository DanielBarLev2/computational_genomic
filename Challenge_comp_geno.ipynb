{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mutation Classification\n",
    "This notebook implements a complete pipeline to classify cancer patients based on their genomic mutation profiles.\n",
    "Using feature engineering, visualization, and a support vector machine classifier, we aim to distinguish between LUSC and HNSC subtypes.\n",
    "The code includes data loading, processing, training, and final prediction for submission.\n",
    "\n",
    "## üìÅ project_root\n",
    "Data should be placed inside root/data folder, as follows:\n",
    "```bash\n",
    "project_root/\n",
    "# ‚îú‚îÄ‚îÄ data/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ train_muts_data.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ test_muts_data.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ train_meth_data.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ test_meth_data.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ train_feats.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ test_feats.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ 100_genes.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ E_cool_ORF.csv\n",
    "# ‚îú‚îÄ‚îÄ plots/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄmutation_types_ordered.png\n",
    "# ‚îú‚îÄ‚îÄ predictions/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ results_muts.csv\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ results_meth.csv\n",
    "# ‚îú‚îÄ‚îÄ Challenge_comp_geno.ipynb\n",
    "```\n",
    "## ‚úÖ Install required packages\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate comp_geno\n",
    "```\n",
    "## ‚öôÔ∏è Config\n"
   ],
   "id": "54c31603d06c77b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PLOT_DIR = PROJECT_ROOT / \"plots\"\n",
    "\n",
    "MUTS_TRAIN_FILE = DATA_DIR / \"train_muts_data.csv\"\n",
    "MUTS_TEST_FILE = DATA_DIR / \"test_muts_data.csv\"\n",
    "METH_TRAIN_FILE = DATA_DIR / \"train_meth_data.csv\"\n",
    "METH_TEST_FILE = DATA_DIR / \"test_meth_data.csv\"\n",
    "\n",
    "PREDICTION_DIR = PROJECT_ROOT / \"predictions\"\n",
    "\n",
    "TRAIN_FEATURE_FILE = DATA_DIR / \"train_feats.csv\"\n",
    "TEST_FEATURE_FILE = DATA_DIR / \"test_feats.csv\"\n",
    "\n",
    "GENE_LIST = DATA_DIR / \"100_genes.csv\"\n",
    "\n",
    "SILENT_CLASSES = {\"Silent\",\n",
    "                  \"Synonymous\",\n",
    "                  \"Intron\",\n",
    "                  \"3'UTR\",\n",
    "                  \"5'UTR\",\n",
    "                  \"IGR\",\n",
    "                  \"Flank\",\n",
    "                  \"FlankNC\"}\n",
    "\n",
    "# All others -> Non-silent\n",
    "GROUP_MAP = [\n",
    "    \"Missense_Mutation\",\n",
    "    \"In_Frame_Del\",\n",
    "    \"Nonsense_Mutation\",\n",
    "    \"Frame_Shift_Del\",\n",
    "    \"Frame_Shift_Ins\",\n",
    "    \"In_Frame_Ins\",\n",
    "    \"Translation_Start_Site\",\n",
    "    \"Nonstop_Mutation\",\n",
    "    \"Splice_Region\",\n",
    "    \"3'UTR\",\n",
    "    \"5'UTR\",\n",
    "    \"Intron\",\n",
    "    \"Splice_Site\",\n",
    "    \"Silent\",\n",
    "    \"3'Flank\",\n",
    "    \"5'Flank\"\n",
    "]"
   ],
   "id": "981ef9aacee07713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Classifier Based on Mutation Features\n",
    "### Section a:\n",
    "#### ‚ú® Feature Overview\n",
    "**build_basic_features** yields the mandatory feature set requested in the assignment:<br>\n",
    "  ‚Ä¢ `total_mutations` - overall mutation burden per patient<br>\n",
    "  ‚Ä¢ `type_<mutation> - ` count of every `Variant_Classification` value<br>\n",
    "  ‚Ä¢ `gene_<symbol>` = mutation count for each of the 100 selected genes<br>\n",
    "\n",
    "### Section b:\n",
    "**build_extra_features** then enriches the matrix with the two ratio features advocated in the npj paper:<br>\n",
    "  ‚Ä¢ `silent_frac` - regulatory ‚Äúsilent‚Äù mutational load, capturing non‚Äëcoding pressure<br>\n",
    "  ‚Ä¢ `del_ratio` - deletion enrichment signal, known to differ across squamous subtypes<br>\n",
    "Including these ratios follows the paper‚Äôs finding that silent load and deletion bias provide compact,<br>\n",
    "informative signals that boost classifier performance.<br>\n",
    "\n",
    "NOTE: we add to these features the given features by concatenation"
   ],
   "id": "e5614ee462bdea76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_basic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build basic features dataframe.\n",
    "    :param df: pandas dataframe - \"train_muts_data.csv\".\n",
    "    return: basic features dataframe:\n",
    "    [total_mutations], [type_counts: for each mutation], [gene_counts, for each mutation].\n",
    "    \"\"\"\n",
    "    order = pd.Index(df['case_id'].unique())\n",
    "\n",
    "    total_mutations = (\n",
    "        df.groupby('case_id', sort=False).size()\n",
    "        .reindex(order)\n",
    "        .rename('total_mutations')\n",
    "    )\n",
    "\n",
    "    type_counts = (\n",
    "        pd.crosstab(df['case_id'], df['Variant_Classification'])\n",
    "        .add_prefix('type_')\n",
    "        .reindex(order, fill_value=0)\n",
    "    )\n",
    "\n",
    "    gene_counts = (\n",
    "        pd.crosstab(df['case_id'], df['Gene_name'])\n",
    "        .add_prefix('gene_')\n",
    "        .reindex(order, fill_value=0)\n",
    "    )\n",
    "\n",
    "    basic_features = pd.concat(\n",
    "        [total_mutations, type_counts, gene_counts],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return basic_features\n",
    "\n",
    "\n",
    "def build_extra_features(df: pd.DataFrame, basic_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build extra features dataframe.\n",
    "    :param df: \"train_muts_data.csv\"\n",
    "    :param basic_features:  basic features dataframe.\n",
    "    :return: extra features dataframe with previous:\n",
    "    [total_mutations], [type_counts: for each mutation], [gene_counts, for each mutation].=\n",
    "    , [silent_frac], [del_ratio].\n",
    "    \"\"\"\n",
    "    order = pd.Index(df['case_id'].unique())\n",
    "\n",
    "    silent_mask = df['Variant_Classification'].isin(SILENT_CLASSES)\n",
    "    silent_counts = (\n",
    "        df[silent_mask].groupby('case_id', sort=False).size()\n",
    "        .reindex(order, fill_value=0)\n",
    "    )\n",
    "    silent_frac = (silent_counts / basic_features['total_mutations']).rename('silent_frac')\n",
    "\n",
    "    alt2 = df['Tumor_Seq_Allele2'].fillna('').str.upper()\n",
    "    ref = df['Reference_Allele'].fillna('').str.upper()\n",
    "    del_mask = (alt2 == '-') | (ref.str.len() > alt2.str.len())\n",
    "\n",
    "    del_counts = (\n",
    "        df[del_mask].groupby('case_id', sort=False).size()\n",
    "        .reindex(order, fill_value=0)\n",
    "    )\n",
    "    del_ratio = (del_counts / basic_features['total_mutations']).rename('del_ratio')\n",
    "\n",
    "    features = pd.concat(\n",
    "        [basic_features, silent_frac, del_ratio],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    count_cols = features.columns.difference(['silent_frac', 'del_ratio'])\n",
    "    features[count_cols] = features[count_cols].fillna(0).astype(int)\n",
    "\n",
    "    return features"
   ],
   "id": "b40a6ad354688933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, compute the basic features and concat them to the given features.<br>\n",
    "The Separation is done to test whether there is an improvement by including the extra features."
   ],
   "id": "951e233ab8e2f706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# compute basic features\n",
    "train_df = pd.read_csv(MUTS_TRAIN_FILE)\n",
    "\n",
    "train_basic_features = build_basic_features(train_df)\n",
    "\n",
    "# load given features\n",
    "given_features = pd.read_csv(TRAIN_FEATURE_FILE)\n",
    "given_features = given_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# concat\n",
    "train_basic_features = pd.concat([given_features.reset_index(drop=True),\n",
    "                                  train_basic_features.reset_index(drop=True)\n",
    "                                  ], axis=1)\n",
    "\n",
    "# separate labels from the data\n",
    "train_basic_features = train_basic_features.drop(columns=[\"case_id\", \"Label\"], errors=\"ignore\")\n",
    "order = train_df[\"case_id\"].unique()\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"case_id\", sort=False)[\"Label\"].first()\n",
    "    .reindex(order)\n",
    ")"
   ],
   "id": "9b36cb253d524af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section c:\n",
    "#### üîç Mutation Distribution Plot and Visualization\n",
    "This utility combines mutation records from both train and test datasets,<br>\n",
    "filters relevant mutation types (based on GROUP_MAP), and plots their counts<br>\n",
    "in a predefined biologically meaningful order. The output is saved as<br>\n",
    "'mutation_types_ordered.png' and can optionally be displayed interactively.<br>"
   ],
   "id": "b745760b0e2b9e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_mutation_distribution(train_df: pd.DataFrame, test_df: pd.DataFrame, show=False) -> None:\n",
    "    \"\"\"\n",
    "    Create and save a bar plot showing mutation counts by Variant_Classification\n",
    "    in a specified order (TCGA-style). Combines train and test datasets.\n",
    "    \"\"\"\n",
    "    out_file = PLOT_DIR / \"mutation_types_ordered.png\"\n",
    "    PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "    mask = df[\"Group\"] = df[\"Variant_Classification\"].isin(GROUP_MAP)\n",
    "    df_filtered = df[mask]\n",
    "\n",
    "    counts = (df_filtered[\"Variant_Classification\"]\n",
    "              .value_counts()\n",
    "              .reindex(GROUP_MAP, fill_value=0)\n",
    "              .rename_axis(\"VC\")\n",
    "              .reset_index(name=\"Count\"))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(counts[\"VC\"], counts[\"Count\"])\n",
    "    plt.xticks(rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"Mutation Count\")\n",
    "    plt.title(\"Mutation types in specified order (GROUP_MAP)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.savefig(out_file, dpi=150)\n",
    "    print(f\"Plot saved to: {out_file}\")\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(MUTS_TEST_FILE)\n",
    "plot_mutation_distribution(train_df, test_df, show=True)"
   ],
   "id": "a8ad2c224c51580d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This plot displays the distribution of mutation types (Variant_Classification) across all patients,<br>\n",
    "using a predefined order that mirrors the presentation in the reference paper.<br>\n",
    "By aligning the categories with GROUP_MAP, we ensure visual consistency with the npj figure, making it easier to compare trends across datasets.<br>\n",
    "This overview helps confirm that the mutation landscape in our cohort is comparable to that described in the original study.<br>\n",
    "\n",
    "Next, an excellent way to represent the data in 2-D is via PCA.<br>\n",
    "Although it does not look like the data is linearly separable by a support vector,<br>\n",
    "The data do have a uniform and consistent structure."
   ],
   "id": "ac9881e779f56c06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(train_basic_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of Mutation Features\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Label\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ff73bb2930d71343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To see how the extra features, silent_frac and del_ratio are behaving,<br>\n",
    "plot them in a scatter plot with corresponding labels.<br>\n",
    "Sadly, we can infer that the new features will not add information for the classification to improve,<br>\n",
    "as will be shown in the next section.<br>\n",
    "Overall, we tried to create even more different features - with no gains.<br>\n",
    "Hence, continued with a simple build."
   ],
   "id": "e7858a6a582a9712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_basic_features = build_basic_features(train_df)\n",
    "train_extra_features = build_extra_features(train_df, train_basic_features)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    train_extra_features['silent_frac'],\n",
    "    train_extra_features['del_ratio'],\n",
    "    c=labels,\n",
    "    cmap='viridis',  # or 'coolwarm', 'plasma', etc.\n",
    "    alpha=0.7,\n",
    "    edgecolors='k'\n",
    ")\n",
    "\n",
    "plt.xlabel('Silent Fraction')\n",
    "plt.ylabel('Deletion Ratio')\n",
    "plt.title('Mutation Features Colored by Cancer Type')\n",
    "plt.colorbar(scatter, label='Label (1 = HNSC, 2 = LUSC)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e7b14fdd19cdc367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section d - Fitting the Data\n",
    "üå≥ Random forest model was chosen since it prevails SVM with different hyperparameters (test was omitted for clearness).<br>\n",
    "A PCA option was added since the data is sparse."
   ],
   "id": "5dfea72667b71d60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, n_estimators=200, random_state=42, use_pca=False, n_components=None):\n",
    "        self.use_pca = use_pca\n",
    "        self.n_components = n_components\n",
    "        self.scaler = StandardScaler()\n",
    "        if use_pca:\n",
    "            self.pca = PCA(n_components=n_components)\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
    "        x_scaled = self.scaler.fit_transform(x)\n",
    "        if self.use_pca:\n",
    "            x_scaled = self.pca.fit_transform(x_scaled)\n",
    "        self.model.fit(x_scaled, y)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, x: pd.DataFrame) -> pd.Series:\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before prediction.\")\n",
    "\n",
    "        x_scaled = self.scaler.transform(x)\n",
    "\n",
    "        if self.use_pca:\n",
    "            x_scaled = self.pca.transform(x_scaled)\n",
    "\n",
    "        return pd.Series(self.model.predict(x_scaled), index=x.index)\n",
    "\n",
    "    def evaluate(self, x: pd.DataFrame, y: pd.Series) -> float:\n",
    "        y_pred = self.predict(x)\n",
    "        return 1 - accuracy_score(y, y_pred)\n"
   ],
   "id": "3128738d0049dd6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This Random Forest class by SKlearn can leverage PCA to preform dimension redaction to squeeze the sparsity into a dense representation.<br> Despite the sparsity, the result with PCA are worse.<br>\n",
    "\n",
    "K-fold validation set is created to mimic a test set.<br>\n",
    "Exactly 20% from the data is dedicated to validation set.<br>\n",
    "\n",
    "Eventually, the model fits the data."
   ],
   "id": "dcc7cdf2a3b27fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = RandomForestClassifierWrapper(use_pca=False, n_components=200)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_basic_features,\n",
    "                                                  labels,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=labels,\n",
    "                                                  random_state=1\n",
    "                                                  )\n",
    "\n",
    "clf.fit(x_train, y_train)"
   ],
   "id": "120f9b1374180066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section e - Validation Set Error\n",
    "‚úÖ The result are great!"
   ],
   "id": "20a21a8bd0eed26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_known_test = pd.DataFrame({\n",
    "    \"predict_label\": clf.predict(x_val),\n",
    "    \"Label\": y_val\n",
    "})\n",
    "error = sum(df_known_test[\"Label\"] != df_known_test[\"predict_label\"]) / len(df_known_test)\n",
    "print(f\"Validation Error Rate: {error:.4f}\")"
   ],
   "id": "93e01ee60f00d778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Despite the current error rate, we try to improve using extra features computed in section b:",
   "id": "e5464d1cb2d8ffb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(MUTS_TRAIN_FILE)\n",
    "\n",
    "# compute basic and extra features\n",
    "train_basic_features = build_basic_features(train_df)\n",
    "train_extra_features = build_extra_features(train_df, train_basic_features)\n",
    "\n",
    "# load given features\n",
    "given_features = pd.read_csv(TRAIN_FEATURE_FILE)\n",
    "given_features = given_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# concat\n",
    "train_extra_features = pd.concat([given_features.reset_index(drop=True),\n",
    "                                  train_extra_features.reset_index(drop=True)\n",
    "                                  ], axis=1)\n",
    "\n",
    "# separate labels from the data\n",
    "train_extra_features = train_extra_features.drop(columns=[\"case_id\", \"Label\"], errors=\"ignore\")\n",
    "order = train_df[\"case_id\"].unique()\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"case_id\", sort=False)[\"Label\"].first()\n",
    "    .reindex(order)\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = RandomForestClassifierWrapper(use_pca=False, n_components=100)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_basic_features,\n",
    "                                                  labels,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=labels,\n",
    "                                                  random_state=1\n",
    "                                                  )\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "df_known_test = pd.DataFrame({\n",
    "    \"predict_label\": clf.predict(x_val),\n",
    "    \"Label\": y_val,\n",
    "})\n",
    "error = sum(df_known_test[\"Label\"] != df_known_test[\"predict_label\"]) / len(df_known_test)\n",
    "print(f\"Validation Error Rate: {error:.4f}\")"
   ],
   "id": "4812e7434fc2a3b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sadly, the results are identical - but still amazing!\n",
    "\n",
    "### Section f - Test set\n",
    "üß™ Finally, predict the caner type for the test cases"
   ],
   "id": "d3160e0420adbf2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(MUTS_TEST_FILE)\n",
    "\n",
    "test_basic_features = build_basic_features(test_df)\n",
    "test_extra_features = build_extra_features(test_df, test_basic_features)\n",
    "\n",
    "given_features = pd.read_csv(TEST_FEATURE_FILE)\n",
    "given_features = given_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "test_extra_features = pd.concat([given_features.reset_index(drop=True),\n",
    "                                 test_extra_features.reset_index(drop=True)\n",
    "                                 ], axis=1)\n",
    "\n",
    "# ensure test features match train columns\n",
    "for col in x_train.columns:\n",
    "    if col not in test_extra_features.columns:\n",
    "        test_extra_features[col] = 0\n",
    "\n",
    "# remove unexpected columns\n",
    "test_extra_features = test_extra_features[x_train.columns]\n",
    "\n",
    "pred = clf.predict(test_extra_features)\n",
    "\n",
    "label_map = {1: \"HNSC\", 2: \"LUSC\"}\n",
    "pred_named = pred.map(label_map)\n",
    "\n",
    "case_ids = test_df[\"case_id\"].unique()\n",
    "\n",
    "predicted_df = pd.DataFrame({\"case_id\": case_ids,\n",
    "                             \"predicted_label\": pred_named.values\n",
    "                             })\n",
    "print(predicted_df.head(10))\n"
   ],
   "id": "c3f75e43664b984d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the predictions in CVS:",
   "id": "c3f9f59db31fb833"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(PREDICTION_DIR, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(PREDICTION_DIR, \"predictions.csv\")\n",
    "predicted_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to: {output_path}\")"
   ],
   "id": "7c188907b35c4797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Classifier Based on Mutation Features\n",
    "### Section a:\n",
    "#### ‚ú® Feature Overview\n",
    "**build_basic_features** yields the mandatory feature set requested in the assignment:<br>\n",
    "  ‚Ä¢ `avg_methylation` - average methylation value for each prob related to a certain gene.<br>\n",
    "\n",
    "\n",
    "### Section b:\n",
    "**build_extra_features** then enriches the matrix with the two ratio features advocated in the npj paper:<br>\n",
    "  ‚Ä¢ `meth_var_per_patient` - variance of all methylation Œ≤-values for each patient,<br>\n",
    "   quantifying heterogeneity of methylation patterns across loci.<br>\n",
    "   High variance may reflect epigenetic instability, which is often correlated with cancer progression and subtype differentiation. <br>\n",
    "  ‚Ä¢ `meth_promoter_density` - fraction of hypermethylated CpG sites in promoter regions (or approximated upstream gene regions).<br>\n",
    "Promoter hypermethylation can lead to gene silencing and thus plays a central role in tumorigenesis.<br>\n",
    "Capturing this density provides a signal about epigenetic silencing across tumor genomes.<br>\n",
    "\n",
    "NOTE: we add to these features the given features by concatenation"
   ],
   "id": "58437b5c3d450f3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_methylation_basic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build methylation basic features dataframe.\n",
    "    :param df: \"train_meth.csv\"\n",
    "    :return: basic features dataframe:\n",
    "    [avg_methylation]\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['matching_genes'])\n",
    "    gene_avg = (df.groupby(['case_id', 'matching_genes'])['beta_val']\n",
    "                .mean()\n",
    "                .unstack(fill_value=0)\n",
    "                .add_prefix('meth_')\n",
    "                )\n",
    "    return gene_avg\n",
    "\n",
    "\n",
    "def build_methylation_extra_features(df: pd.DataFrame, basic_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build methylation extra features dataframe.\n",
    "    :param df: \"train_meth.csv\" / \"test_meth.csv\"\n",
    "    :return: extra features dataframe:\n",
    "    [meth_var], [promoter_density]\n",
    "    \"\"\"\n",
    "    meth_var = df.groupby('case_id')['beta_val'].var().rename('meth_var_per_patient')\n",
    "\n",
    "    is_promoter = df['matching_genes'].str.contains('^P|FLANK|flank|prom', case=False, na=False)\n",
    "    high_beta = df['beta_val'] > 0.7\n",
    "    hyper_meth = df[is_promoter & high_beta].groupby('case_id').size()\n",
    "    total_meth = df.groupby('case_id').size()\n",
    "    promoter_density = (hyper_meth / total_meth).fillna(0).rename('meth_promoter_density')\n",
    "\n",
    "    extra = pd.concat([meth_var, promoter_density], axis=1)\n",
    "    return basic_features.join(extra, how='left').fillna(0)"
   ],
   "id": "33e392ec086eab79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, compute the basic features and concat them to the given features.<br>\n",
    "Again, The Separation is done to test whether there is an improvement by including the extra features."
   ],
   "id": "c5b87b8d04666a73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# compute basic features\n",
    "train_df = pd.read_csv(METH_TRAIN_FILE)\n",
    "\n",
    "train_basic_features = build_methylation_basic_features(train_df)\n",
    "train_extra_features = build_methylation_extra_features(train_df, train_basic_features)\n",
    "\n",
    "# load given features\n",
    "given_features = pd.read_csv(TRAIN_FEATURE_FILE)\n",
    "given_features = given_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# concat\n",
    "train_basic_features = pd.concat([given_features.reset_index(drop=True),\n",
    "                                  train_basic_features.reset_index(drop=True)\n",
    "                                  ], axis=1)\n",
    "\n",
    "# enrich with extra features\n",
    "train_basic_features = build_methylation_extra_features(train_df, train_basic_features)\n",
    "\n",
    "# separate labels from the data\n",
    "train_basic_features = train_basic_features.drop(columns=[\"case_id\", \"Label\"], errors=\"ignore\")\n",
    "order = train_df[\"case_id\"].unique()\n",
    "\n",
    "labels = (\n",
    "    train_df.groupby(\"case_id\", sort=False)[\"Label\"].first()\n",
    "    .reindex(order)\n",
    ")\n"
   ],
   "id": "2d6230813f77964e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using PCA to visualize the data, we can infer that the given features are dominant.<br>\n",
    "Thus, we will classify the data with the extra features only."
   ],
   "id": "98e4eb5e0d81abde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(train_basic_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"PCA of Mutation Features\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Label\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c75c9b55cdb3e833",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To see how the extra features, **meth_promoter_density** and **meth_var_per_patient** are behaving,<br>\n",
    "plot them in a scatter plot with corresponding labels.<br>"
   ],
   "id": "a65df430444984ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_basic_features = build_methylation_basic_features(train_df)\n",
    "train_extra_features = build_methylation_extra_features(train_df, train_basic_features)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(\n",
    "    train_extra_features['meth_var_per_patient'],\n",
    "    train_extra_features['meth_promoter_density'],\n",
    "    c=labels,\n",
    "    cmap='viridis',  # or 'coolwarm', 'plasma', etc.\n",
    "    alpha=0.7,\n",
    "    edgecolors='k'\n",
    ")\n",
    "\n",
    "plt.xlabel('meth_var_per_patient Fraction')\n",
    "plt.ylabel('meth_promoter_density Ratio')\n",
    "plt.title('methylation Features Colored by Cancer Type')\n",
    "plt.colorbar(scatter, label='Label (1 = HNSC, 2 = LUSC)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "649d4cbf3222fe54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Section d - Fitting the Data",
   "id": "aedd52ae11776767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clf = RandomForestClassifierWrapper(use_pca=False, n_components=100)\n",
    "clf.fit(x_train, y_train)"
   ],
   "id": "2d6e019abf629e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section e - Validation Set Error\n",
    "‚úÖ The result are great!"
   ],
   "id": "44fe6d7a8a543473"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_known_test = pd.DataFrame({\n",
    "    \"predict_label\": clf.predict(x_val),\n",
    "    \"Label\": y_val\n",
    "})\n",
    "error = sum(df_known_test[\"Label\"] != df_known_test[\"predict_label\"]) / len(df_known_test)\n",
    "print(f\"Validation Error Rate: {error:.4f}\")"
   ],
   "id": "380321e04adbedac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Section f - Test set",
   "id": "a0bce78627b3ee01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(METH_TEST_FILE)\n",
    "\n",
    "test_basic_features = build_methylation_basic_features(test_df)\n",
    "test_extra_features = build_methylation_extra_features(test_df, test_basic_features)\n",
    "\n",
    "given_features = pd.read_csv(TEST_FEATURE_FILE)\n",
    "given_features = given_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "test_extra_features = pd.concat([\n",
    "    given_features.reset_index(drop=True),\n",
    "    test_extra_features.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "missing_cols = [col for col in x_train.columns if col not in test_extra_features.columns]\n",
    "filled_missing = pd.DataFrame(0, index=test_extra_features.index, columns=missing_cols)\n",
    "\n",
    "test_extra_features = pd.concat([test_extra_features, filled_missing], axis=1)\n",
    "test_extra_features = test_extra_features[x_train.columns]\n",
    "\n",
    "pred = clf.predict(test_extra_features)\n",
    "\n",
    "label_map = {1: \"HNSC\", 2: \"LUSC\"}\n",
    "pred_named = pred.map(label_map)\n",
    "\n",
    "case_ids = test_df[\"case_id\"].unique()\n",
    "\n",
    "predicted_df = pd.DataFrame({\"case_id\": case_ids,\n",
    "                             \"predicted_label\": pred_named.values})\n",
    "print(predicted_df.head(10))\n"
   ],
   "id": "d3556aa76a9949f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the predictions in CVS:\n",
   "id": "4ecbd19bf328a50f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(PREDICTION_DIR, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(PREDICTION_DIR, \"predictions2.csv\")\n",
    "predicted_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to: {output_path}\")"
   ],
   "id": "459273bb544a2894",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
